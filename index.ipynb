{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model Validation - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll be able to validate your Ames Housing data model using train-test split.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Compare training and testing errors to determine if model is over or underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use our Ames Housing Data again!\n",
    "\n",
    "We included the code to preprocess below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ames = pd.read_csv('ames.csv')\n",
    "\n",
    "continuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n",
    "categoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n",
    "\n",
    "ames_cont = ames[continuous]\n",
    "\n",
    "# log features\n",
    "log_names = [f'{column}_log' for column in ames_cont.columns]\n",
    "\n",
    "ames_log = np.log(ames_cont)\n",
    "ames_log.columns = log_names\n",
    "\n",
    "# normalize (subract mean and divide by std)\n",
    "\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "ames_log_norm = ames_log.apply(normalize)\n",
    "\n",
    "# one hot encode categoricals\n",
    "ames_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n",
    "\n",
    "preprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed.drop('SalePrice_log', axis=1)\n",
    "y = preprocessed['SalePrice_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ames = pd.read_csv('ames.csv')\n",
    "\n",
    "continuous = ['LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n",
    "categoricals = ['BldgType', 'KitchenQual', 'SaleType', 'MSZoning', 'Street', 'Neighborhood']\n",
    "\n",
    "ames_cont = ames[continuous]\n",
    "\n",
    "# log features\n",
    "log_names = [f'{column}_log' for column in ames_cont.columns]\n",
    "\n",
    "ames_log = np.log(ames_cont)\n",
    "ames_log.columns = log_names\n",
    "\n",
    "# normalize (subract mean and divide by std)\n",
    "\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "ames_log_norm = ames_log.apply(normalize)\n",
    "\n",
    "# one hot encode categoricals\n",
    "ames_ohe = pd.get_dummies(ames[categoricals], prefix=categoricals, drop_first=True)\n",
    "\n",
    "preprocessed = pd.concat([ames_log_norm, ames_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "X = preprocessed.drop('SalePrice_log', axis=1)\n",
    "y = preprocessed['SalePrice_log']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets. Use the default split size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "# A brief preview of our train test split\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply your model to the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and initialize the linear regression model class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predictions on training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions on training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate training and test residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "train_residuals = y_hat_train - y_train\n",
    "test_residuals = y_hat_test - y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Mean Squared Error (MSE)\n",
    "\n",
    "A good way to compare overall performance is to compare the mean squarred error for the predicted values on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squarred Error:', train_mse)\n",
    "print('Test Mean Squarred Error:', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your test error is substantially worse than the train error, this is a sign that the model doesn't generalize well to future cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to demonstrate overfitting and underfitting is to alter the size of our train-test split. By default, scikit-learn allocates 25% of the data to the test set and 75% to the training set. Fitting a model on only 10% of the data is apt to lead to underfitting, while training a model on 99% of the data is apt to lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the effect of train-test split size\n",
    "\n",
    "Iterate over a range of train-test split sizes from .5 to .95. For each of these, generate a new train/test split sample. Fit a model to the training sample and calculate both the training error and the test error (mse) for each of these splits. Plot these two curves (train error vs. training size and test error vs. training size) on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "import random\n",
    "random.seed(110)\n",
    "\n",
    "train_err = []\n",
    "test_err = []\n",
    "t_sizes = list(range(5,100,5))\n",
    "for t_size in t_sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_size/100)\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_hat_train = linreg.predict(X_train)\n",
    "    y_hat_test = linreg.predict(X_test)\n",
    "    train_err.append(mean_squared_error(y_train, y_hat_train))\n",
    "    test_err.append(mean_squared_error(y_test, y_hat_test))\n",
    "plt.scatter(t_sizes, train_err, label='Training Error')\n",
    "plt.scatter(t_sizes, test_err, label='Testing Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the effect of train-test split size: Extension\n",
    "\n",
    "Repeat the previous example, but for each train-test split size, generate 10 iterations of models/errors and save the average train/test error. This will help account for any particularly good/bad models that might have resulted from poor/good splits in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(900)\n",
    "\n",
    "train_err = []\n",
    "test_err = []\n",
    "t_sizes = range(5,100,5)\n",
    "for t_size in t_sizes:\n",
    "    temp_train_err = []\n",
    "    temp_test_err = []\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t_size/100)\n",
    "        linreg.fit(X_train, y_train)\n",
    "        y_hat_train = linreg.predict(X_train)\n",
    "        y_hat_test = linreg.predict(X_test)\n",
    "        temp_train_err.append(mean_squared_error(y_train, y_hat_train))\n",
    "        temp_test_err.append(mean_squared_error(y_test, y_hat_test))\n",
    "    train_err.append(np.mean(temp_train_err))\n",
    "    test_err.append(np.mean(temp_test_err))\n",
    "plt.scatter(t_sizes, train_err, label='Training Error')\n",
    "plt.scatter(t_sizes, test_err, label='Testing Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here? Evaluate your result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now practiced your knowledge of MSE and used your train-test split skills to validate your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
